[
    {
        "Name": "neural_training_data_compression",
        "Title": "Investigating Data Compression Techniques for Neural Network Training: A Comparative Study of Information Preservation and Model Performance",
        "Experiment": "Explore and evaluate different compression methods (DCT, Random Projection, Downsampling, Binary Mask) for reducing neural network training data storage requirements while maintaining model performance. The study begins with MNIST dataset compression using DCT transformation, followed by implementing and comparing alternative compression schemes. Key research questions include: (1) Compression efficiency and accuracy trade-offs across different methods, (2) Relationship between compression ratio and model performance, (3) Information theoretic analysis of compressed representations, (4) Potential for dataset reconstruction or identification from compressed forms. The experiment will measure compression ratios, model accuracy on compressed data versus original data, and analyze the information-theoretic properties of compressed representations. Additional investigation will explore the cryptographic implications of compressed data uniqueness and the possibility of dataset identification through compressed representations. This research aims to establish optimal compression strategies for reducing storage and bandwidth requirements in neural network training while maintaining model performance, with potential applications in efficient AI training and data privacy.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    },
    {
        "Name": "learnable_compression_masks",
        "Title": "Learning Task-Specific Compression Patterns: A Two-Phase Approach to Adaptive Data Compression",
        "Experiment": "1. Train baseline model on uncompressed data. 2. Modify CompressedDataset to include a trainable mask initialized to ones. 3. Freeze model weights and train mask using gradient descent to minimize (loss + \u03bb||mask||\u2081), starting from different sparsity multipliers \u03bb. 4. For each \u03bb, record accuracy vs compression ratio curve. 5. Validate mask generalization by: (a) training on 20% of data and testing on rest, (b) comparing masks learned from different data subsets. 6. Analyze coefficient importance by computing correlation between mask values and accuracy impact. Key outputs: optimal sparsity-accuracy trade-off curves, visualization of learned masks, generalization metrics.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "novel": true
    }
]