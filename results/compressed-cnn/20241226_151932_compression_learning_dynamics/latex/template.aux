\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Deng2020ModelCA}
\citation{Li2023ModelCF}
\citation{wang2022learning}
\citation{Fort2020DeepLV}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{wang2022learning}
\citation{Hegde2007EfficientML}
\citation{azimi2020structural}
\citation{Fort2020DeepLV}
\citation{Bingham2001RandomPI}
\citation{Deng2020ModelCA}
\citation{wang2022learning}
\citation{Gupta1999AnEP}
\citation{Herrmann2022ChaoticDA}
\citation{Horoi2021ExploringTG}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}{Related Work}{section.2}{}}
\newlabel{sec:related@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.3}\protected@file@percent }
\newlabel{sec:background}{{3}{2}{Background}{section.3}{}}
\newlabel{sec:background@cref}{{[section][3][]3}{[1][2][]2}}
\citation{LeCun1998GradientbasedLA}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Setting}{3}{subsection.3.1}\protected@file@percent }
\newlabel{subsec:problem}{{3.1}{3}{Problem Setting}{subsection.3.1}{}}
\newlabel{subsec:problem@cref}{{[subsection][1][3]3.1}{[1][2][]3}}
\newlabel{eq:dct}{{1}{3}{Problem Setting}{equation.3.1}{}}
\newlabel{eq:dct@cref}{{[equation][1][]1}{[1][3][]3}}
\newlabel{eq:rp}{{2}{3}{Problem Setting}{equation.3.2}{}}
\newlabel{eq:rp@cref}{{[equation][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{section.4}\protected@file@percent }
\newlabel{sec:method}{{4}{3}{Method}{section.4}{}}
\newlabel{sec:method@cref}{{[section][4][]4}{[1][3][]3}}
\newlabel{eq:grad_norm}{{3}{3}{Method}{equation.4.3}{}}
\newlabel{eq:grad_norm@cref}{{[equation][3][]3}{[1][3][]3}}
\newlabel{eq:grad_corr}{{4}{3}{Method}{equation.4.4}{}}
\newlabel{eq:grad_corr@cref}{{[equation][4][]4}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{3}{section.5}\protected@file@percent }
\newlabel{sec:experimental}{{5}{3}{Experimental Setup}{section.5}{}}
\newlabel{sec:experimental@cref}{{[section][5][]5}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{4}{section.6}\protected@file@percent }
\newlabel{sec:results}{{6}{4}{Results}{section.6}{}}
\newlabel{sec:results@cref}{{[section][6][]6}{[1][4][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance comparison across compression methods and learning rates, showing random projections' consistent accuracy and reduced training time. Standard errors computed over 5 runs.\relax }}{4}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:performance}{{1}{4}{Performance comparison across compression methods and learning rates, showing random projections' consistent accuracy and reduced training time. Standard errors computed over 5 runs.\relax }{table.caption.1}{}}
\newlabel{tab:performance@cref}{{[table][1][]1}{[1][4][]4}}
\newlabel{fig:loss_train}{{1a}{4}{Training Loss\relax }{figure.caption.2}{}}
\newlabel{fig:loss_train@cref}{{[subfigure][1][1]1a}{[1][4][]4}}
\newlabel{sub@fig:loss_train}{{a}{4}{Training Loss\relax }{figure.caption.2}{}}
\newlabel{sub@fig:loss_train@cref}{{[subfigure][1][1]1a}{[1][4][]4}}
\newlabel{fig:loss_val}{{1b}{4}{Validation Loss\relax }{figure.caption.2}{}}
\newlabel{fig:loss_val@cref}{{[subfigure][2][1]1b}{[1][4][]4}}
\newlabel{sub@fig:loss_val}{{b}{4}{Validation Loss\relax }{figure.caption.2}{}}
\newlabel{sub@fig:loss_val@cref}{{[subfigure][2][1]1b}{[1][4][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Loss trajectories showing random projections' faster convergence and lower variance across learning rates compared to DCT methods.\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:training_curves}{{1}{4}{Loss trajectories showing random projections' faster convergence and lower variance across learning rates compared to DCT methods.\relax }{figure.caption.2}{}}
\newlabel{fig:training_curves@cref}{{[figure][1][]1}{[1][4][]4}}
\bibstyle{iclr2024_conference}
\bibdata{references}
\bibcite{azimi2020structural}{{1}{2020}{{Azimi \& Pekcan}}{{Azimi and Pekcan}}}
\bibcite{Bingham2001RandomPI}{{2}{2001}{{Bingham \& Mannila}}{{Bingham and Mannila}}}
\bibcite{Deng2020ModelCA}{{3}{2020}{{Deng et~al.}}{{Deng, Li, Han, Shi, and Xie}}}
\bibcite{Fort2020DeepLV}{{4}{2020}{{Fort et~al.}}{{Fort, Dziugaite, Paul, Kharaghani, Roy, and Ganguli}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Test accuracy comparison demonstrating random projections' consistent 97.68\% performance versus DCT's 95.58\% across learning rates.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:accuracy}{{2}{5}{Test accuracy comparison demonstrating random projections' consistent 97.68\% performance versus DCT's 95.58\% across learning rates.\relax }{figure.caption.3}{}}
\newlabel{fig:accuracy@cref}{{[figure][2][]2}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{5}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{5}{Conclusions}{section.7}{}}
\newlabel{sec:conclusion@cref}{{[section][7][]7}{[1][5][]5}}
\bibcite{Gupta1999AnEP}{{5}{1999}{{Gupta \& Dasgupta}}{{Gupta and Dasgupta}}}
\bibcite{Hegde2007EfficientML}{{6}{2007}{{Hegde et~al.}}{{Hegde, Davenport, Wakin, and Baraniuk}}}
\bibcite{Herrmann2022ChaoticDA}{{7}{2022}{{Herrmann et~al.}}{{Herrmann, Granz, and Landgraf}}}
\bibcite{Horoi2021ExploringTG}{{8}{2021}{{Horoi et~al.}}{{Horoi, chun Huang, Rieck, Lajoie, Wolf, and Krishnaswamy}}}
\bibcite{LeCun1998GradientbasedLA}{{9}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{Li2023ModelCF}{{10}{2023}{{Li et~al.}}{{Li, Li, and Meng}}}
\bibcite{wang2022learning}{{11}{2022}{{Wang et~al.}}{{Wang, Qin, and Chen}}}
\ttl@finishall
