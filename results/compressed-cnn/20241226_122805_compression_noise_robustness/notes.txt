# Title: Understanding Neural Network Robustness Through Noisy Compressed Representations
# Experiment description: Modify CompressedDataset to include Gaussian noise injection in the compressed domain. Study two key variables: (1) Compression ratio by varying DCT coefficient mask size (8x8, 16x16, 24x24), (2) Noise magnitude (σ = 0.1, 0.2, 0.3). Compare models trained on: clean compressed data vs noisy compressed data. Measure: (1) Clean test accuracy, (2) Noisy test accuracy, (3) Accuracy degradation rate with increasing noise. Implementation requires adding noise_level parameter to CompressedDataset, creating different DCT masks, and extending evaluation to track performance under noise. Analyze how compression ratio affects natural noise tolerance.

## Run 0: Baseline (No compression, no noise)
Results: {'mnist': {'best_val_acc_mean': 95.58, 'test_acc_mean': 95.58, 'total_train_time_mean': 827.2352156639099}}
Description: Baseline results using standard MNIST without compression or noise. Achieved strong performance of 95.58% accuracy, establishing a good reference point for subsequent experiments.

## Run 1: Initial DCT Compression (8x8) with Noise (σ=0.2)
Results: {'mnist': {'best_val_acc_mean': 34.82, 'clean_test_acc_mean': 34.82, 'noisy_test_acc_mean': 34.82, 'total_train_time_mean': 854.905177116394}}
Description: First attempt at training with 8x8 DCT compression and Gaussian noise (σ=0.2). The significant drop in accuracy (34.82%) compared to baseline (95.58%) suggests the noise level may be too high for initial experiments. The identical clean and noisy test accuracies indicate the model might be struggling with the basic compressed representation before even considering noise effects. For Run 2, we will reduce the noise magnitude to σ=0.1 while keeping the 8x8 DCT compression to better understand the impact of noise level on model performance.

## Run 2: DCT Compression (8x8) with Reduced Noise (σ=0.1)
Results: {'mnist': {'best_val_acc_mean': 39.77, 'clean_test_acc_mean': 39.77, 'noisy_test_acc_mean': 39.77, 'total_train_time_mean': 817.5519616603851}}
Description: Second attempt using 8x8 DCT compression with reduced noise (σ=0.1). The slight improvement in accuracy (39.77% vs 34.82%) suggests that while reducing noise helped somewhat, the model is still performing poorly. The identical clean and noisy test accuracies again indicate fundamental issues with the compressed representation itself. For Run 3, we will attempt training with compression only (no noise) to isolate whether the compression or the noise is the primary limiting factor.

## Run 3: DCT Compression (8x8) without Noise
Results: {'mnist': {'best_val_acc_mean': 44.82, 'clean_test_acc_mean': 44.82, 'noisy_test_acc_mean': 44.82, 'total_train_time_mean': 777.234982252121}}
Description: Third attempt using 8x8 DCT compression with no noise injection. The improved accuracy (44.82% vs 39.77%) confirms that noise was indeed hampering performance, but the still-low accuracy indicates that the 8x8 DCT compression is too aggressive, discarding too much important image information. The identical clean and noisy test accuracies (both 44.82%) suggest the model has learned a stable but limited representation of the data. For Run 4, we will increase the DCT mask size to 16x16 (no noise) to preserve more image information and potentially improve performance.

## Run 4: DCT Compression (16x16) without Noise
Results: {'mnist': {'best_val_acc_mean': 95.58, 'clean_test_acc_mean': 95.58, 'noisy_test_acc_mean': 95.58, 'total_train_time_mean': 951.9686357975006}}
Description: Fourth attempt using 16x16 DCT compression with no noise injection. The dramatic improvement in accuracy (95.58% vs 44.82%) demonstrates that 16x16 DCT compression preserves sufficient information for near-baseline performance (matching the 95.58% of Run 0). This suggests we've found an appropriate compression level that maintains essential image features while still providing meaningful dimensionality reduction. For Run 5, we will maintain the 16x16 DCT compression and introduce moderate noise (σ=0.1) to test if this larger representation space provides better noise robustness than the 8x8 compression.

# Visualization Analysis

Three key plots were generated to analyze the experimental results:

## 1. Training Loss Plot (train_loss_mnist_across_runs.png)
This figure shows the training loss curves for all experimental runs over training iterations. The x-axis represents training iterations, while the y-axis shows the cross-entropy loss value. The shaded areas around each line represent the standard error across multiple seeds, indicating the stability of training. Key observations:
- Baseline and 16x16 DCT (Run 0 and Run 4) show similar, rapid convergence
- 8x8 DCT runs (Runs 1-3) show consistently higher loss values
- Noise injection (Runs 1, 2, and 5) results in more volatile training curves
- The 16x16 mask size (Runs 4-5) enables significantly lower training loss compared to 8x8 compression

## 2. Validation Loss Plot (val_loss_mnist_across_runs.png)
This visualization tracks the validation loss across training iterations for all runs. Similar to the training loss plot, but evaluated on the validation set. Notable findings:
- Clear separation between high-performing (Runs 0, 4) and struggling configurations (Runs 1-3)
- Validation curves are smoother than training curves due to batch averaging
- Noise injection creates larger gaps between training and validation performance
- 16x16 compression maintains validation stability similar to the baseline

## 3. Test Accuracy Bar Chart (test_accuracy_mnist_across_runs.png)
This bar chart provides a final performance comparison across all runs, showing test accuracy percentages. Key insights:
- Baseline (Run 0) and 16x16 DCT without noise (Run 4) achieve identical 95.58% accuracy
- Progressive improvement from Run 1 (34.82%) to Run 4 (95.58%)
- Clear visualization of how mask size affects model performance
- Impact of noise level (σ) on final accuracy
- Demonstrates the trade-off between compression ratio and model performance

These visualizations collectively tell the story of how DCT compression mask size and noise injection affect model training dynamics and final performance. They clearly show that 16x16 compression preserves sufficient information for optimal performance, while 8x8 compression is too aggressive for this task.
